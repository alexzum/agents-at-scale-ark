# Mock OpenAI server using mock-llm (https://github.com/dwmkerr/mock-llm)
# Returns the system message (messages[0]) to validate agent prompt parameter resolution
apiVersion: v1
kind: ConfigMap
metadata:
  name: mock-llm-config
data:
  mock-llm.yaml: |
    rules:
      - path: "/v1/chat/completions"
        match: "@"
        response:
          status: 200
          content: |
            {
              "id": "mock-{{timestamp}}",
              "object": "chat.completion",
              "model": "{{jmes request model}}",
              "choices": [{
                "message": {{jmes request messages[0]}},
                "finish_reason": "stop"
              }]
            }
---
apiVersion: v1
kind: Service
metadata:
  name: mock-openai
spec:
  selector:
    app: mock-openai
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: v1
kind: Pod
metadata:
  name: mock-openai
  labels:
    app: mock-openai
spec:
  containers:
  - name: mock-llm
    image: ghcr.io/dwmkerr/mock-llm:0.1.8
    ports:
    - containerPort: 8080
    volumeMounts:
    - name: config
      mountPath: /app/mock-llm.yaml
      subPath: mock-llm.yaml
    readinessProbe:
      httpGet:
        path: /health
        port: 8080
      initialDelaySeconds: 2
      periodSeconds: 5
  volumes:
  - name: config
    configMap:
      name: mock-llm-config
