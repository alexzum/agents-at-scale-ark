# Mock LLM configuration for query parameter resolution test
# Custom rule returns messages[0] (system message with resolved query parameters)
ark:
  model:
    enabled: true
    name: mock-gpt-4.1
    type: openai
    model: gpt-4.1
    pollInterval: 3s
    apiKey: mock-api-key

config:
  rules:
  - path: "/v1/chat/completions"
    match: "@"
    response:
      status: 200
      content: |
        {
          "id": "mock-{{timestamp}}",
          "object": "chat.completion",
          "model": "{{jmes request body.model}}",
          "choices": [{
            "message": {{jmes request body.messages[0]}},
            "finish_reason": "stop"
          }]
        }
