# Mock LLM server using mock-llm helm chart
# This will echo back all messages sent to it, allowing us to verify message delivery
apiVersion: v1
kind: ConfigMap
metadata:
  name: mock-llm-config
data:
  mock-llm.yaml: |
    rules:
      - path: "/v1/chat/completions"
        match: "@"
        response:
          status: 200
          content: |
            {
              "id": "mock-{{timestamp}}",
              "object": "chat.completion",
              "model": "{{jmes request model}}",
              "choices": [{
                "message": {
                  "role": "assistant",
                  "content": "MOCK_RESPONSE: {{jmes request messages}}"
                },
                "finish_reason": "stop"
              }],
              "usage": {
                "prompt_tokens": 100,
                "completion_tokens": 50,
                "total_tokens": 150
              }
            }
---
apiVersion: v1
kind: Service
metadata:
  name: mock-llm
spec:
  selector:
    app: mock-llm
  ports:
  - port: 80
    targetPort: 8080
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: mock-llm
spec:
  replicas: 1
  selector:
    matchLabels:
      app: mock-llm
  template:
    metadata:
      labels:
        app: mock-llm
    spec:
      containers:
      - name: mock-llm
        image: ghcr.io/dwmkerr/mock-llm:0.1.8
        ports:
        - containerPort: 8080
        volumeMounts:
        - name: config
          mountPath: /app/mock-llm.yaml
          subPath: mock-llm.yaml
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 2
          periodSeconds: 5
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 10
      volumes:
      - name: config
        configMap:
          name: mock-llm-config
