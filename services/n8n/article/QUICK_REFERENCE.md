# Quick Reference: ARK + n8n Integration

## Article Assets Summary

All files located in: `services/n8n/article/`

### ARK Resources
- `01-model.yaml` - GPT-4 model configuration
- `02-agent.yaml` - Customer support agent
- `03-evaluator.yaml` - Response quality evaluator

### Workflow
- `n8n-workflow.json` - Complete workflow (import into n8n)

### Documentation
- `README.md` - Setup and testing instructions
- `ARTICLE.md` - Full Medium article draft
- `SCREENSHOT_GUIDE.md` - Screenshot capture instructions
- `QUICK_REFERENCE.md` - This file

---

## Deploy & Test Commands

### Deploy ARK Resources
```bash
kubectl apply -f services/n8n/article/01-model.yaml
kubectl apply -f services/n8n/article/02-agent.yaml
kubectl apply -f services/n8n/article/03-evaluator.yaml
kubectl wait --for=condition=ready agent/support-agent --timeout=60s
```

### Test High Quality Response
```bash
curl -X POST http://n8n.default.127.0.0.1.nip.io:8080/webhook/customer-support \
  -H "Content-Type: application/json" \
  -d '{
    "customer_name": "Alice Johnson",
    "customer_email": "alice@example.com",
    "account_type": "Enterprise",
    "priority": "high",
    "issue": "I cannot access the API dashboard. When I try to log in, I get a 403 error. I have verified my credentials are correct."
  }'
```

### Test Low Quality Response
```bash
curl -X POST http://n8n.default.127.0.0.1.nip.io:8080/webhook/customer-support \
  -H "Content-Type: application/json" \
  -d '{
    "customer_name": "Bob Smith",
    "customer_email": "bob@example.com",
    "account_type": "Free",
    "priority": "low",
    "issue": "Why is everything broken?"
  }'
```

### Check Results
```bash
# View ARK resources
kubectl get agent,model,evaluator

# View evaluations
kubectl get evaluation

# View specific evaluation details
kubectl describe evaluation <evaluation-name>
```

---

## Key Concepts for Article

### The Problem
Organizations struggle to build production-grade AI agent workflows that are observable, quality-controlled, and easy to iterate.

### The Solution
ARK (Kubernetes-native AI orchestration) + n8n (workflow automation) + custom nodes = Production-ready AI workflows with quality gates.

### The Innovation
Four custom n8n nodes that expose ARK resources:
1. **ARK Agent** - Execute AI agents
2. **ARK Model** - Direct LLM calls
3. **ARK Team** - Multi-agent coordination
4. **ARK Evaluation** - Quality gates

### The Differentiator
**Built-in evaluation framework** that scores every AI output before it reaches production.

---

## Article Structure

1. **Hook** (The Challenge)
2. **Solution** (ARK + n8n)
3. **What We Built** (4 nodes)
4. **Real-World Example** (Customer support)
5. **Implementation** (Step-by-step)
6. **Advanced Features** (Resource locators, query evaluation)
7. **Architecture** (How it works)
8. **Benefits** (Dev/Ops/Business value)
9. **Real Impact** (Metrics)
10. **Getting Started** (Quick setup)
11. **Roadmap** (What's next)
12. **Call to Action** (Try it, community)

---

## Key Messages

### For Developers
âœ… Visual debugging of AI interactions
âœ… Rapid iteration without code changes
âœ… Version control with GitOps

### For Operations
âœ… Kubernetes-native infrastructure
âœ… Scalable and observable
âœ… Secure with RBAC

### For Business
âœ… Quality gates prevent bad AI outputs
âœ… Compliance through evaluation criteria
âœ… Risk mitigation with human-in-the-loop

---

## Workflow Value Proposition

**Traditional AI Integration**:
- Write code â†’ Deploy â†’ Hope it works â†’ Fix issues in production

**ARK + n8n Integration**:
- Visual workflow â†’ Quality gates â†’ Test â†’ Deploy with confidence

**The Difference**:
Every AI response is evaluated before reaching customers.

---

## Screenshot Priorities

### Must Have:
1. Full workflow canvas
2. ARK Agent node config
3. ARK Evaluation node with advanced parameters
4. High quality execution showing score
5. Low quality execution showing review path

### Nice to Have:
6. Resource locator modes
7. kubectl resources list
8. IF node condition logic
9. Architecture diagram

---

## Example Narratives for Social Media

### Twitter/X Thread
```
ðŸš€ We built production-grade AI workflows by combining ARK (K8s-native agents) with @n8n.

The game-changer? Built-in quality gates for every AI output.

Here's how we did it ðŸ§µ

[1/8]
```

### LinkedIn Post
```
Tired of AI agents that work great in demos but fail in production?

We solved this by extending ARK with n8n to create a platform where every AI response is evaluated before reaching customers.

94% of responses meet quality thresholds.
6% are caught and reviewed by humans.
Zero inappropriate responses reached customers.

Here's the architecture and code ðŸ‘‡
```

### Reddit r/MachineLearning
```
[P] Built custom n8n nodes for Kubernetes-native AI agent orchestration with quality gates

We integrated ARK (Agents at Scale) with n8n to create a workflow platform where AI agents are evaluated before production impact.

Key innovation: ARK Evaluation node that scores responses across multiple dimensions (relevance, accuracy, clarity, compliance) and routes based on quality thresholds.

Full code + example workflow available. Feedback welcome!
```

---

## Common Questions & Answers

**Q: Why not just use LangChain?**
A: ARK provides Kubernetes-native infrastructure with built-in evaluation framework. LangChain is great for development; ARK is built for production operations.

**Q: Why n8n instead of Airflow/Prefect?**
A: n8n's visual interface and 400+ integrations make it ideal for non-technical stakeholders to modify workflows. Plus, it's open source.

**Q: How does evaluation scaling work?**
A: Evaluations run as separate Kubernetes jobs, scaling independently from agents and workflows.

**Q: Can I use different LLM providers?**
A: Yes! ARK supports OpenAI, Azure OpenAI, Anthropic, AWS Bedrock, and more through unified Model CRD.

**Q: What about evaluation costs?**
A: Evaluations use LLM calls, so there's cost. But catching one bad customer interaction saves more than evaluation costs for thousands of good ones.

---

## Publishing Checklist

Before publishing:

- [ ] All screenshots captured and inserted
- [ ] Code snippets tested and verified
- [ ] Links updated (GitHub, docs, community)
- [ ] Author bio and contact info added
- [ ] Tags added: #AI #Kubernetes #n8n #Agents #LLM #Evaluation
- [ ] SEO: Title, description, preview image
- [ ] Proofread for typos and grammar
- [ ] Test all kubectl commands
- [ ] Test all curl commands
- [ ] Verify workflow JSON imports correctly
- [ ] Cross-post to LinkedIn, Twitter, Reddit
- [ ] Share in relevant Slack/Discord communities

---

## Follow-Up Content Ideas

### Blog Series:
1. This article (Introduction + basic example)
2. "Advanced ARK Evaluation Patterns" (batch, baseline, query-driven)
3. "Building Multi-Agent Teams with ARK" (team coordination)
4. "GitOps for AI Agents" (version control, CI/CD)
5. "Cost Optimization for LLM Workflows" (model selection, caching)

### Video Content:
- 5-minute demo walkthrough
- Live coding: Building an agent from scratch
- Deep dive: Evaluation framework internals

### Webinar:
"Production AI: From Prototype to Production in 30 Minutes"

---

## Metrics to Track

After publishing, track:
- Views, reads, read ratio
- Claps/reactions
- Comments and questions
- GitHub stars/forks increase
- Community Slack signups
- Workflow imports (if tracked)
- Social media engagement

Goal: Establish thought leadership in production AI orchestration.
